{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch\n",
    "\n",
    "\n",
    "Author: [Anand Saha](http://teleported.in/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Accessing popular datasets via torchvision and torchtext\n",
    "\n",
    "**Objective**: Learn how to access popular datasets via PyTorch utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's access the CIFAR10 dataset via torchvision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms are applied to each image. Some are mandatory, like conversion to Tensor and \n",
    "# Normalize the image. Others are optional and used for data augmentation (like random crop, \n",
    "# random jitter etc.)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                # Various transforms can be added in this pipeline for data augmentation\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.utils.data.Dataset`\n",
    "\n",
    "All datasets (like CIFAR10) are inherited from torch.utils.data.Dataset abstract class. `torch.utils.data.Dataset` represents a dataset, and via the `__len__()` and `__iter__()` functions allows it's client to iterate through the dataset. Custom implementations would read custom datasets and provide a standard interface to it's clients.\n",
    "\n",
    "In the below example, the CIFAR10 class did the work for us in loading CIFAR10 data. It has mechanisms to download the dataset from source, unpack it, and provide us the necessary iterators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "len(trainset.train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_data\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainset\u001b[38;5;241m.\u001b[39mtrain_labels[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Happens to be the truck class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'train_data'"
     ]
    }
   ],
   "source": [
    "plt.imshow(trainset.train_data[1])\n",
    "print(trainset.train_labels[1], \" - Happens to be the truck class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.utils.data.DataLoader`\n",
    "\n",
    "DataLoader can load multiple samples parallelly. It provides an iterater to fetch one batch at a time, with the batchsize as specified.\n",
    "\n",
    "DataLoader helps in:\n",
    "* Batching the data\n",
    "* Shuffling the data\n",
    "* Load the data in parallel using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training loop skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0\n",
      "\n",
      "type(data):  <class 'torch.Tensor'>\n",
      "data.size():  torch.Size([10, 3, 32, 32])\n",
      "\n",
      "type(labels):  <class 'torch.Tensor'>\n",
      "labels.size():  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "\n",
    "    data, labels = data\n",
    "    \n",
    "    print(\"Iteration \", i)\n",
    "    print(\"\")\n",
    "    print(\"type(data): \", type(data))\n",
    "    print(\"data.size(): \", data.size())\n",
    "    print(\"\")\n",
    "    print(\"type(labels): \", type(labels))\n",
    "    print(\"labels.size(): \", labels.size())\n",
    "    \n",
    "    # Model training happens here\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets available**\n",
    "\n",
    "* MNIST\n",
    "* Fashion-MNIST\n",
    "* EMNIST\n",
    "* COCO\n",
    "* Captions\n",
    "* Detection\n",
    "* LSUN\n",
    "* ImageFolder\n",
    "* Imagenet-12\n",
    "* CIFAR\n",
    "* STL10\n",
    "* SVHN\n",
    "* PhotoTour"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
